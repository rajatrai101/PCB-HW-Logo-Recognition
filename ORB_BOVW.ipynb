{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "import numpy as np \n",
    "from glob import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noOfSamples\n",
    "noOfClasses=19\n",
    "testPath = './../fics-logoaugmentator/split/val/'\n",
    "trainPath = './../fics-logoaugmentator/split/train/'\n",
    "imgSize= 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageHelpers:\n",
    "\tdef __init__(self):\n",
    "\t\tself.sift_object = cv2.xfeatures2d.SIFT_create()\n",
    "\t\n",
    "\tdef scale(im):\n",
    "\t\treturn cv2.resize(im, (imgSize,imgSize))\n",
    "\t\t\n",
    "\tdef gray(self, image):\n",
    "\t\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\t\treturn scale(gray)\n",
    "\n",
    "\tdef features(self, image):\n",
    "\t\tkeypoints, descriptors = self.sift_object.detectAndCompute(image, None)\n",
    "\t\treturn [keypoints, descriptors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOVHelpers:\n",
    "\tdef __init__(self, n_clusters = noOfClasses):\n",
    "\t\tself.n_clusters = n_clusters\n",
    "\t\tself.kmeans_obj = KMeans(n_clusters = n_clusters)\n",
    "\t\tself.kmeans_ret = None\n",
    "\t\tself.descriptor_vstack = None\n",
    "\t\tself.mega_histogram = None\n",
    "\t\tself.clf  = SVC()\t\n",
    "\n",
    "\tdef cluster(self):\n",
    "\t\t\"\"\"\t\n",
    "\t\tcluster using KMeans algorithm, \n",
    "\t\t\"\"\"\n",
    "\t\tself.kmeans_ret = self.kmeans_obj.fit_predict(self.descriptor_vstack)\n",
    "\n",
    "\tdef developVocabulary(self,n_images, descriptor_list, kmeans_ret = None):\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tEach cluster denotes a particular visual word \n",
    "\t\tEvery image can be represeted as a combination of multiple \n",
    "\t\tvisual words. The best method is to generate a sparse histogram\n",
    "\t\tthat contains the frequency of occurence of each visual word \n",
    "\t\tThus the vocabulary comprises of a set of histograms of encompassing\n",
    "\t\tall descriptions for all images\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tself.mega_histogram = np.array([np.zeros(self.n_clusters) for i in range(n_images)])\n",
    "\t\told_count = 0\n",
    "\t\tfor i in range(len(descriptor_list)):\n",
    "\t\t\tl = len(descriptor_list[i])\n",
    "\t\t\tfor j in range(l):\n",
    "\t\t\t\tif kmeans_ret is None:\n",
    "\t\t\t\t\tidx = self.kmeans_ret[old_count+j]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tidx = kmeans_ret[old_count+j]\n",
    "\t\t\t\tself.mega_histogram[i][idx] += 1\n",
    "\t\t\told_count += l\n",
    "\t\tprint(\"Vocabulary Histogram Generated\")\n",
    "\n",
    "\tdef standardize(self, std=None):\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tstandardize is required to normalize the distribution\n",
    "\t\twrt sample size and features. If not normalized, the classifier may become\n",
    "\t\tbiased due to steep variances.\n",
    "\t\t\"\"\"\n",
    "\t\tif std is None:\n",
    "\t\t\tself.scale = StandardScaler().fit(self.mega_histogram)\n",
    "\t\t\tself.mega_histogram = self.scale.transform(self.mega_histogram)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"STD not none. External STD supplied\")\n",
    "\t\t\tself.mega_histogram = std.transform(self.mega_histogram)\n",
    "\n",
    "\tdef formatND(self, l):\n",
    "\t\t\"\"\"\t\n",
    "\t\trestructures list into vstack array of shape\n",
    "\t\tM samples x N features for sklearn\n",
    "\t\t\"\"\"\n",
    "\t\tvStack = np.array(l[0])\n",
    "\t\tfor remaining in l[1:]:\n",
    "\t\t\tvStack = np.vstack((vStack, remaining))\n",
    "\t\tself.descriptor_vstack = vStack.copy()\n",
    "\t\treturn vStack\n",
    "\n",
    "\tdef train(self, train_labels):\n",
    "\t\t\"\"\"\n",
    "\t\tuses sklearn.svm.SVC classifier (SVM) \n",
    "\t\t\"\"\"\n",
    "\t\tprint(\"Training SVM\")\n",
    "\t\tprint (self.clf)\n",
    "\t\tprint(\"Train labels\", train_labels)\n",
    "\t\tself.clf.fit(self.mega_histogram, train_labels)\n",
    "\t\tprint (\"Training completed\")\n",
    "\n",
    "\tdef predict(self, iplist):\n",
    "\t\tpredictions = self.clf.predict(iplist)\n",
    "\t\treturn predictions\n",
    "\n",
    "\tdef plotHist(self, vocabulary = None):\n",
    "\t\tprint (\"Plotting histogram\")\n",
    "\t\tif vocabulary is None:\n",
    "\t\t\tvocabulary = self.mega_histogram\n",
    "\n",
    "\t\tx_scalar = np.arange(self.n_clusters)\n",
    "\t\ty_scalar = np.array([abs(np.sum(vocabulary[:,h], dtype=np.int32)) for h in range(self.n_clusters)])\n",
    "\n",
    "\t\t# print (y_scalar)\n",
    "\n",
    "\t\tplt.bar(x_scalar, y_scalar)\n",
    "\t\tplt.xlabel(\"Visual Word Index\")\n",
    "\t\tplt.ylabel(\"Frequency\")\n",
    "\t\tplt.title(\"Complete Vocabulary Generated\")\n",
    "\t\tplt.xticks(x_scalar + 0.4, x_scalar)\n",
    "\t\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileHelpers:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef getFiles(self, path):\n",
    "\t\t\"\"\"\n",
    "\t\t- returns  a dictionary of all files \n",
    "\t\thaving key => value as  objectname => image path\n",
    "\t\t- returns total number of files.\n",
    "\t\t\"\"\"\n",
    "\t\timlist = {}\n",
    "\t\tclassCount,count = 0,0\n",
    "\t\tfor each in glob(path + \"*\"):\n",
    "\t\t\tword = (each.split(\"\\\\\")[-1]).strip()\n",
    "\t\t\tprint(\" #### Reading image category\", word, \" ##### \")\n",
    "\t\t\timlist[word] = []\n",
    "\t\t\t# print(path+word+\"\\*\")\n",
    "\t\t\tfor imagefile in glob(path+word+\"\\*\"):\n",
    "\t\t\t\t# print (\"Reading file \", imagefile)\n",
    "\t\t\t\tim = cv2.imread(imagefile, 0)\n",
    "# \t\t\t\tprint(im.shape)\n",
    "# \t\t\t\tplt.figure()\n",
    "# \t\t\t\tplt.imshow(im,'gray')\n",
    "\t\t\t\timlist[word].append(im)\n",
    "\t\t\t\tcount +=1 \n",
    "\t\t\tclassCount +=1 \n",
    "\t\treturn [imlist, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_fh = FileHelpers()\n",
    "# imglist,count = test_fh.getFiles(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(len(imglist),count)\n",
    "# imgll=[len(k) for k in imglist.values()]\n",
    "# print(min(imgll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOV:\n",
    "    def __init__(self, no_clusters):\n",
    "        self.no_clusters = no_clusters\n",
    "        self.train_path = None\n",
    "        self.test_path = None\n",
    "        self.im_helper = ImageHelpers()\n",
    "        # self.bov_helper = BOVHelpers()\n",
    "        self.bov_helper = BOVHelpers(no_clusters)\n",
    "        \n",
    "        self.file_helper = FileHelpers()\n",
    "        self.images = None\n",
    "        self.trainImageCount = 0\n",
    "        self.train_labels = np.array([])\n",
    "        self.name_dict = {}\n",
    "        self.descriptor_list = []\n",
    "\n",
    "    def trainModel(self):\n",
    "        \"\"\"\n",
    "        This method contains the entire module \n",
    "        required for training the bag of visual words model\n",
    "        Use of helper functions will be extensive.\n",
    "        \"\"\"\n",
    "\n",
    "        # read file. prepare file lists.\n",
    "        self.images, self.trainImageCount = self.file_helper.getFiles(self.train_path)\n",
    "        # extract SIFT Features from each image\n",
    "        label_count = 0 \n",
    "        for word, imlist in self.images.items():\n",
    "            self.name_dict[str(label_count)] = word\n",
    "            print(\"Computing Features for \", word)\n",
    "            for im in imlist:\n",
    "        \n",
    "                self.train_labels = np.append(self.train_labels, label_count)\n",
    "                kp, des = self.im_helper.features(im)\n",
    "                if des is not None:\n",
    "                    self.descriptor_list.append(des)\n",
    "\n",
    "            label_count += 1\n",
    "\n",
    "\n",
    "        # perform clustering\n",
    "        bov_descriptor_stack = self.bov_helper.formatND(self.descriptor_list)\n",
    "        self.bov_helper.cluster()\n",
    "        self.bov_helper.developVocabulary(n_images = self.trainImageCount, descriptor_list=self.descriptor_list)\n",
    "\n",
    "        # show vocabulary trained\n",
    "        self.bov_helper.plotHist()\n",
    " \n",
    "\n",
    "        self.bov_helper.standardize()\n",
    "        self.bov_helper.train(self.train_labels)\n",
    "\n",
    "\n",
    "    def recognize(self,test_img, test_image_path=None):\n",
    "\n",
    "        \"\"\" \n",
    "        This method recognizes a single image \n",
    "        It can be utilized individually as well.\n",
    "        \"\"\"\n",
    "\n",
    "        kp, des = self.im_helper.features(test_img)\n",
    "        # print kp\n",
    "        # print(des.shape)\n",
    "\n",
    "        # generate vocab for test image\n",
    "        vocab = np.array( [[ 0 for i in range(noOfClasses)]])\n",
    "        # locate nearest clusters for each of \n",
    "        # the visual word (feature) present in the image\n",
    "        \n",
    "        # test_ret =<> return of kmeans nearest clusters for N features\n",
    "        test_ret = self.bov_helper.kmeans_obj.predict(des)\n",
    "        # print(test_ret)\n",
    "\n",
    "        # print vocab\n",
    "        for each in test_ret:\n",
    "            # if each<18:\n",
    "                vocab[0][each] += 1\n",
    "\n",
    "        # print(vocab)\n",
    "        # Scale the features\n",
    "        vocab = self.bov_helper.scale.transform(vocab)\n",
    "\n",
    "        # predict the class of the image\n",
    "        lb = self.bov_helper.clf.predict(vocab)\n",
    "        # print(\"Image belongs to class : \", self.name_dict[str(int(lb[0]))])\n",
    "        return lb\n",
    "\n",
    "\n",
    "\n",
    "    def testModel(self):\n",
    "        \"\"\" \n",
    "        This method is to test the trained classifier\n",
    "        read all images from testing path \n",
    "        use BOVHelpers.predict() function to obtain classes of each image\n",
    "        \"\"\"\n",
    "        print(\"Using Test Path:\",self.test_path)\n",
    "        self.testImages, self.testImageCount = self.file_helper.getFiles(self.test_path)\n",
    "\n",
    "        predictions = []\n",
    "        y_test,y_pred=[],[]\n",
    "        label_count,total,correct=0,0,0\n",
    "        for word, imlist in self.testImages.items():\n",
    "            print(\"processing ...\" ,word)\n",
    "            for im in imlist:\n",
    "                # print imlist[0].shape, imlist[1].shape\n",
    "                # print(im.shape)\n",
    "                if im is not None:\n",
    "                    cl = self.recognize(im)\n",
    "                    # print(cl[0])\n",
    "                    predictions.append({\n",
    "                        'image':im,\n",
    "                        'class':cl,\n",
    "                        'object_name':self.name_dict[str(int(cl[0]))],\n",
    "                        'real_name':self.name_dict[str(int(label_count))]                        \n",
    "                        })\n",
    "                    y_test+=[label_count]\n",
    "                    y_pred+=[cl[0]]\n",
    "                    total+=1\n",
    "                    if cl[0] == label_count:\n",
    "                        correct+=1\n",
    "            label_count += 1\n",
    "\n",
    "        # print(predictions)\n",
    "        # for each in predictions:\n",
    "        #     cv2.imshow(each['real_name']+\"->\"+each['object_name'], each['image'])\n",
    "        #     cv2.waitKey()\n",
    "        #     cv2.destroyWindow(each['object_name'])\n",
    "            \n",
    "        #     plt.imshow(cv2.cvtColor(each['image'], cv2.COLOR_GRAY2RGB))\n",
    "        #     plt.title(each['object_name'])\n",
    "        #     plt.show()\n",
    "        precision, recall, f1, support = prfs(y_test, y_pred, average='weighted')\n",
    "        print(\"Precision: {:.2%}\\nRecall: {:.2%}\\nF1 score: {:.2%}\".format(precision, recall, f1))\n",
    "        print(\"Accuracy :\",(correct/total)*100,\"%\")\n",
    "\n",
    "\n",
    "    def print_vars(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bov = BOV(noOfClasses)\n",
    "# set training paths\n",
    "bov.train_path = trainPath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " #### Reading image category Altera  ##### \n",
      " #### Reading image category Analog Devices  ##### \n",
      " #### Reading image category Bel fuse inc  ##### \n",
      " #### Reading image category Fairchild Semiconductor  ##### \n",
      " #### Reading image category General Semiconductors Industries Inc  ##### \n",
      " #### Reading image category Holtek Semiconductors  ##### \n",
      " #### Reading image category Level One  ##### \n",
      " #### Reading image category Linear Technologies  ##### \n",
      " #### Reading image category Micron Technologies  ##### \n",
      " #### Reading image category Mitsubishi Electric Corporation  ##### \n",
      " #### Reading image category Motorola Semiconductor Products Inc  ##### \n",
      " #### Reading image category Nvidia  ##### \n",
      " #### Reading image category ON Semiconductors  ##### \n",
      " #### Reading image category Pericom Semiconductors  ##### \n",
      " #### Reading image category Realtek Semiconductors  ##### \n",
      " #### Reading image category Recongized Component Mark  ##### \n",
      " #### Reading image category STMicroelectronics  ##### \n",
      " #### Reading image category Texas Instruments Inc  ##### \n",
      " #### Reading image category Xilinx  ##### \n",
      "Computing Features for  Altera\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.5.1) c:\\users\\appveyor\\appdata\\local\\temp\\1\\pip-req-build-i1s8y2i1\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x67dbbff5::Set<1,-1,-1>,struct cv::impl::A0x67dbbff5::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-bebb325f0483>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-bfd68bd1e751>\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mkp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim_helper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescriptor_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-5db0f8131404>\u001b[0m in \u001b[0;36mfeatures\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                 \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                 \u001b[0mkeypoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescriptors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msift_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeypoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescriptors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-5db0f8131404>\u001b[0m in \u001b[0;36mgray\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mgray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                 \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) c:\\users\\appveyor\\appdata\\local\\temp\\1\\pip-req-build-i1s8y2i1\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x67dbbff5::Set<1,-1,-1>,struct cv::impl::A0x67dbbff5::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "bov.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set testing paths\n",
    "print(testPath)\n",
    "bov.test_path = testPath\n",
    "bov.testModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}